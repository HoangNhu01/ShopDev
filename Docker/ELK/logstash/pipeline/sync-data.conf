input {
  kafka {
    bootstrap_servers => "10.0.0.24:29092"
    topics => ["server1.sd_inventory.Shop"]
    codec => json {
      target => "data"  # Specify the target field for JSON data
    }
  }
}

filter {
  if [payload][before] {
    # Handle update case
    mutate {
      add_field => {
        "id" => "%{[payload][Id]}"
      }
      add_field => {
        "name" => "%{[payload][Name]}"
      }
      add_field => { "operation" => "update" }
      remove_field => ["data", "@version", "@timestamp", "message", "event", "globalId", "schema", "source", "op", "ts_ms", "transaction"]
    }
    # Add logic here to handle updates (e.g., using a script to merge)
  } else if [payload][after] {
    # Handle insert case
    mutate {
      add_field => {
        "id" => "%{[payload][Id]}"
      }
      add_field => {
        "name" => "%{[payload][Name]}"
      }
      add_field => { "operation" => "insert" }
      remove_field => ["data", "@version", "@timestamp", "message", "event", "globalId", "schema", "source", "op", "ts_ms", "transaction"]
    }
  }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
        hosts => "${ELASTICSEARCH_HOSTS}"
        ssl_enabled => false
        # ssl_certificate_authorities => "${ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES}"
        user => "${ELASTICSEARCH_USERNAME}"
        password => "${ELASTICSEARCH_PASSWORD}"
        index => "sql_product"
        #document_id => "%{Id}" #chọn id cho document bằng một trường cụ thể
        #action => "update"
        doc_as_upsert => true
    }
}
